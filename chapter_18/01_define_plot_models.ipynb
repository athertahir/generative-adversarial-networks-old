{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# create and plot the infogan model for mnist\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Activation\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "# define the standalone discriminator model\n",
        "def define_discriminator(n_cat, in_shape=(28,28,1)):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# image input\n",
        "\tin_image = Input(shape=in_shape)\n",
        "\t# downsample to 14x14\n",
        "\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n",
        "\td = LeakyReLU(alpha=0.1)(d)\n",
        "\t# downsample to 7x7\n",
        "\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "\td = LeakyReLU(alpha=0.1)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\t# normal\n",
        "\td = Conv2D(256, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "\td = LeakyReLU(alpha=0.1)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\t# flatten feature maps\n",
        "\td = Flatten()(d)\n",
        "\t# real/fake output\n",
        "\tout_classifier = Dense(1, activation='sigmoid')(d)\n",
        "\t# define d model\n",
        "\td_model = Model(in_image, out_classifier)\n",
        "\t# compile d model\n",
        "\td_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
        "\t# create q model layers\n",
        "\tq = Dense(128)(d)\n",
        "\tq = BatchNormalization()(q)\n",
        "\tq = LeakyReLU(alpha=0.1)(q)\n",
        "\t# q model output\n",
        "\tout_codes = Dense(n_cat, activation='softmax')(q)\n",
        "\t# define q model\n",
        "\tq_model = Model(in_image, out_codes)\n",
        "\treturn d_model, q_model\n",
        "\n",
        "# define the standalone generator model\n",
        "def define_generator(gen_input_size):\n",
        "\t# weight initialization\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\t# image generator input\n",
        "\tin_lat = Input(shape=(gen_input_size,))\n",
        "\t# foundation for 7x7 image\n",
        "\tn_nodes = 512 * 7 * 7\n",
        "\tgen = Dense(n_nodes, kernel_initializer=init)(in_lat)\n",
        "\tgen = Activation('relu')(gen)\n",
        "\tgen = BatchNormalization()(gen)\n",
        "\tgen = Reshape((7, 7, 512))(gen)\n",
        "\t# normal\n",
        "\tgen = Conv2D(128, (4,4), padding='same', kernel_initializer=init)(gen)\n",
        "\tgen = Activation('relu')(gen)\n",
        "\tgen = BatchNormalization()(gen)\n",
        "\t# upsample to 14x14\n",
        "\tgen = Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
        "\tgen = Activation('relu')(gen)\n",
        "\tgen = BatchNormalization()(gen)\n",
        "\t# upsample to 28x28\n",
        "\tgen = Conv2DTranspose(1, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(gen)\n",
        "\t# tanh output\n",
        "\tout_layer = Activation('tanh')(gen)\n",
        "\t# define model\n",
        "\tmodel = Model(in_lat, out_layer)\n",
        "\treturn model\n",
        "\n",
        "# define the combined discriminator, generator and q network model\n",
        "def define_gan(g_model, d_model, q_model):\n",
        "\t# make weights in the discriminator (some shared with the q model) as not trainable\n",
        "\td_model.trainable = False\n",
        "\t# connect g outputs to d inputs\n",
        "\td_output = d_model(g_model.output)\n",
        "\t# connect g outputs to q inputs\n",
        "\tq_output = q_model(g_model.output)\n",
        "\t# define composite model\n",
        "\tmodel = Model(g_model.input, [d_output, q_output])\n",
        "\t# compile model\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss=['binary_crossentropy', 'categorical_crossentropy'], optimizer=opt)\n",
        "\treturn model\n",
        "\n",
        "# number of values for the categorical control code\n",
        "n_cat = 10\n",
        "# size of the latent space\n",
        "latent_dim = 62\n",
        "# create the discriminator\n",
        "d_model, q_model = define_discriminator(n_cat)\n",
        "# create the generator\n",
        "gen_input_size = latent_dim + n_cat\n",
        "g_model = define_generator(gen_input_size)\n",
        "# create the gan\n",
        "gan_model = define_gan(g_model, d_model, q_model)\n",
        "# plot the model\n",
        "plot_model(gan_model, to_file='gan_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}